Federated learning has rapidly gained traction due to its promise of privacy-preserving, decentralized model training. However, its distributed nature also opens the door to novel vulnerabilities such as backdoor attacks, where a small fraction of malicious clients can inject subtle trigger patterns into their local data. These triggers cause the global model to misclassify inputs containing the trigger, while maintaining high accuracy on clean data. For instance, Bagdasaryan et al. (2020) demonstrated that with as few as 10–20\% compromised clients, attack success rates could soar to 80–90\% in targeted backdoor scenarios, highlighting a serious security risk in federated settings. Such attacks become even more challenging to detect when adversaries employ strategies to vary trigger attributes (e.g., size or location invariance), making standard anomaly detection less effective.

In response, researchers have explored a range of defenses. Robust aggregation methods, such as Krum, Multi-Krum, and coordinate-wise median, have been proposed to limit the influence of outlier updates, although their performance can degrade under non-IID data distributions common in federated learning. Other promising defenses include anomaly detection frameworks (e.g., Neural Cleanse) and differential privacy techniques that obscure malicious gradients while preserving model utility. Despite these advances, a trade-off often emerges between mitigating backdoor effectiveness and maintaining clean accuracy, thereby motivating ongoing research into adaptive and dynamic defense mechanisms. Notable studies such as those by Bhagoji et al. (2019) and Bagdasaryan et al. (2020) provide empirical statistics and insights that underscore both the severity of the threat and the challenges inherent in devising robust countermeasures.


References:
B. Wang et al., "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks," 2019 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, USA, 2019, pp. 707-723, doi: 10.1109/SP.2019.00031. keywords: {Training;Biological neural networks;Face recognition;Face;Neurons;Computational modeling;Security;Deep-Learning;Security;Backdoor-Attack},

Bhagoji, A.N., Chakraborty, S., Mittal, P. \&; Calo, S.. (2019). Analyzing Federated Learning through an Adversarial Lens. Proceedings of the 36th International Conference on Machine Learning, in Proceedings of Machine Learning Research 97:634-643 Available from https:\/\/proceedings.mlr.press\/v97\/bhagoji19a.html.

Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D. \&; Shmatikov, V.. (2020). How To Backdoor Federated Learning. Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, in Proceedings of Machine Learning Research 108:2938-2948 Available from https:\/\/proceedings.mlr.press\/v108\/bagdasaryan20a.html.